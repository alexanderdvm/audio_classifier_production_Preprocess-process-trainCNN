{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4cba1a-7866-4a1f-a93e-2520adac96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkeo de librerias completOo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import subprocess\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "print(\"checkeo de librerias completOo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f623cf01-e94c-4362-9035-5bb99938c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU configurada con l√≠mite 3481 MB\n",
      "check\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# ---- VARIABLES ---------\n",
    "# ------------------------\n",
    "DATA_DIR = r\"D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation\"\n",
    "OUTPUT_ROOT = r\"D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\"\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "# Audio / features\n",
    "SR = 4000\n",
    "DURATION = 10\n",
    "SAMPLES = SR * DURATION\n",
    "N_MFCC = 40\n",
    "N_MELS = 40\n",
    "HOP_LENGTH = 256\n",
    "\n",
    "# Entrenamiento / CV\n",
    "N_FOLDS = 5\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Modelo\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT = 0.25\n",
    "\n",
    "# Cache\n",
    "CACHE_DIR = os.path.join(OUTPUT_ROOT, 'feature_cache')\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "FEATURE_TYPES = ['mfcc', 'mel', 'concat']  # tipos de features\n",
    "\n",
    "# GPU\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "if physical_gpus:\n",
    "    try:\n",
    "        try:\n",
    "            out = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'])\n",
    "            total_mb = int(out.decode('utf-8').strip().split('\\n')[0])\n",
    "            limit_mb = int(total_mb * 0.85)\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                physical_gpus[0],\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=limit_mb)])\n",
    "            print(f\" GPU configurada con l√≠mite {limit_mb} MB\")\n",
    "        except Exception:\n",
    "            tf.config.experimental.set_memory_growth(physical_gpus[0], True)\n",
    "            print(\" nvidia-smi no disponible: activado memory_growth\")\n",
    "    except Exception as e:\n",
    "        print(' No se pudo configurar GPU:', e)\n",
    "else:\n",
    "    print(' No se detect√≥ GPU: ejecutando en CPU')\n",
    "\n",
    "print(\"check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e031ef88-33f6-46a9-a804-40e566bfbafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se crearon las funciones correctamenteE\n"
     ]
    }
   ],
   "source": [
    "#recorrido de carpetas\n",
    "def list_files_and_labels(root_dir):\n",
    "    filepaths, labels = [], []\n",
    "    classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(root_dir, cls)\n",
    "        for ext in ('*.wav', '*.WAV'):\n",
    "            for f in glob(os.path.join(cls_dir, ext)):\n",
    "                filepaths.append(f)\n",
    "                labels.append(cls)\n",
    "    return filepaths, labels\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    return path\n",
    "\n",
    "print(\"se crearon las funciones correctamenteE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bedd468-81fb-43fb-b024-108fc6ecc57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se crearon las funciones correctamente\n"
     ]
    }
   ],
   "source": [
    "#extraccion de caracteristicas\n",
    "def extract_mfcc(filepath, sr=SR, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, duration=DURATION):\n",
    "    y, _ = librosa.load(filepath, sr=sr, duration=duration)\n",
    "    if len(y) < SAMPLES: y = np.pad(y, (0, SAMPLES - len(y)))\n",
    "    else: y = y[:SAMPLES]\n",
    "    return librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length)\n",
    "\n",
    "def extract_mel(filepath, sr=SR, n_mels=N_MELS, hop_length=HOP_LENGTH, duration=DURATION):\n",
    "    y, _ = librosa.load(filepath, sr=sr, duration=duration)\n",
    "    if len(y) < SAMPLES: y = np.pad(y, (0, SAMPLES - len(y)))\n",
    "    else: y = y[:SAMPLES]\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
    "    return librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "def pad_feature(feat, target_frames):\n",
    "    if feat.shape[1] < target_frames:\n",
    "        pad_width = target_frames - feat.shape[1]\n",
    "        feat = np.pad(feat, ((0,0),(0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        feat = feat[:, :target_frames]\n",
    "    return feat\n",
    "\n",
    "print(\"se crearon las funciones correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653c9ccc-adc2-4401-8922-ed274be9d00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1423 audios copiados a D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\test_web para prueba web\n"
     ]
    }
   ],
   "source": [
    "# Listado completo, SEPARAR EN 95/5%\n",
    "filepaths, labels = list_files_and_labels(DATA_DIR)\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(labels)\n",
    "class_names = list(le.classes_)\n",
    "\n",
    "# Split 95%-5%\n",
    "X_train_files, X_test_web_files, y_train_labels, y_test_web_labels = train_test_split(\n",
    "    filepaths, labels, test_size=0.05, stratify=labels, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Crear carpeta test_web\n",
    "test_web_dir = os.path.join(OUTPUT_ROOT, 'test_web')\n",
    "os.makedirs(test_web_dir, exist_ok=True)\n",
    "\n",
    "# Copiar audios de prueba web\n",
    "for fp, lbl in zip(X_test_web_files, y_test_web_labels):\n",
    "    class_dir = os.path.join(test_web_dir, lbl)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    shutil.copy2(fp, class_dir)\n",
    "\n",
    "print(f\" {len(X_test_web_files)} audios copiados a {test_web_dir} para prueba web\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c45050-eafc-4a31-b938-5950a47eacbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Calculando mfcc para 27025 archivos y guardando en 2 partes...\n",
      "  ‚Üí Guardada parte 1/2 en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_mfcc_part1.npz (X.shape=(13513, 40, 157, 1))\n",
      "  ‚Üí Guardada parte 2/2 en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_mfcc_part2.npz (X.shape=(13512, 40, 157, 1))\n",
      "‚Üí Guardado cache en partes y metadata para mfcc en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_mfcc_metadata.json\n",
      "cargado cacheo\n",
      "‚Üí Calculando mel para 27025 archivos y guardando en 2 partes...\n",
      "  ‚Üí Guardada parte 1/2 en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_mel_part1.npz (X.shape=(13513, 40, 157, 1))\n",
      "  ‚Üí Guardada parte 2/2 en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_mel_part2.npz (X.shape=(13512, 40, 157, 1))\n",
      "‚Üí Guardado cache en partes y metadata para mel en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_mel_metadata.json\n",
      "cargado cacheo\n",
      "‚Üí Calculando concat para 27025 archivos y guardando en 2 partes...\n",
      "  ‚Üí Guardada parte 1/2 en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_concat_part1.npz (X.shape=(13513, 80, 157, 1))\n",
      "  ‚Üí Guardada parte 2/2 en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_concat_part2.npz (X.shape=(13512, 80, 157, 1))\n",
      "‚Üí Guardado cache en partes y metadata para concat en D:\\dataset pf\\COPIA\\organized_by_type\\FILTERED_AUDIO_10s\\data_augmentation_train2\\feature_cache\\features_concat_metadata.json\n",
      "cargado cacheo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Par√°metro: cu√°ntas partes crear (aj√∫stalo seg√∫n memoria disponible)\n",
    "N_PARTS = 2  # <-- cambia este valor seg√∫n tus recursos\n",
    "\n",
    "# ejemplo y TARGET_FRAMES ya definidos antes\n",
    "_example_mfcc = extract_mfcc(X_train_files[0])\n",
    "TARGET_FRAMES = _example_mfcc.shape[1]\n",
    "\n",
    "cached_paths = {}\n",
    "for ftype in FEATURE_TYPES:\n",
    "    # base path (ya que ahora habr√° varias partes)\n",
    "    cp_base = os.path.join(CACHE_DIR, f'features_{ftype}')\n",
    "    # lista de posibles archivos por parte\n",
    "    part_paths = [f\"{cp_base}_part{idx+1}.npz\" for idx in range(N_PARTS)]\n",
    "    # guardaremos en cached_paths info √∫til (manteniendo la variable cached_paths)\n",
    "    cached_paths[ftype] = {'path': cp_base, 'parts': part_paths, 'X': None, 'y': None}\n",
    "\n",
    "# funci√≥n auxiliar para cargar/concatenar partes s√≥lo cuando se necesite\n",
    "def load_cached(ftype, concat=True):\n",
    "    \"\"\"\n",
    "    Carga las partes del cache para 'ftype'.\n",
    "    Si concat==True retorna (X,y) concatenados (y en int encoded).\n",
    "    Si concat==False retorna listas (X_parts, y_parts) con cada parte por separado.\n",
    "    Adem√°s setea cached_paths[ftype]['X'] y ['y'] si concat=True.\n",
    "    \"\"\"\n",
    "    info = cached_paths[ftype]\n",
    "    parts = info['parts']\n",
    "    X_parts = []\n",
    "    y_parts = []\n",
    "    any_exist = False\n",
    "\n",
    "    for p in parts:\n",
    "        if os.path.exists(p):\n",
    "            any_exist = True\n",
    "            data = np.load(p, allow_pickle=True)\n",
    "            Xp, yp = data['X'], data['y']\n",
    "            X_parts.append(Xp)\n",
    "            y_parts.append(yp)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if not any_exist:\n",
    "        return None if concat else ([], [])\n",
    "\n",
    "    if concat:\n",
    "        X = np.concatenate(X_parts, axis=0)\n",
    "        y = np.concatenate(y_parts, axis=0)\n",
    "        cached_paths[ftype]['X'] = X\n",
    "        cached_paths[ftype]['y'] = y\n",
    "        return X, y\n",
    "    else:\n",
    "        return X_parts, y_parts\n",
    "\n",
    "# Procesar y crear partes (si no existen ya)\n",
    "for ftype in FEATURE_TYPES:\n",
    "    part_paths = cached_paths[ftype]['parts']\n",
    "    meta_path = cached_paths[ftype]['path'] + \"_metadata.json\"\n",
    "\n",
    "    if any(os.path.exists(p) for p in part_paths):\n",
    "        print(f'‚Üí Cargando cache (partes) de {ftype} desde {cached_paths[ftype][\"path\"]} (partes detectadas)')\n",
    "    else:\n",
    "        print(f'‚Üí Calculando {ftype} para {len(X_train_files)} archivos y guardando en {N_PARTS} partes...')\n",
    "        indices = np.arange(len(X_train_files))\n",
    "        splits = np.array_split(indices, N_PARTS)\n",
    "\n",
    "        le_local = LabelEncoder()\n",
    "        all_labels = [lbl for lbl in y_train_labels]\n",
    "        le_local.fit(all_labels)\n",
    "\n",
    "        total_samples = 0\n",
    "        for part_idx, idx_chunk in enumerate(splits):\n",
    "            feats = []\n",
    "            labels_part = []\n",
    "            for i in idx_chunk:\n",
    "                fp = X_train_files[i]\n",
    "                lbl = y_train_labels[i]\n",
    "\n",
    "                if ftype == 'mfcc':\n",
    "                    f = extract_mfcc(fp)\n",
    "                elif ftype == 'mel':\n",
    "                    f = extract_mel(fp)\n",
    "                elif ftype == 'concat':\n",
    "                    f = np.vstack([extract_mfcc(fp), extract_mel(fp)])\n",
    "                else:\n",
    "                    raise ValueError(f\"Feature type desconocido: {ftype}\")\n",
    "\n",
    "                f = pad_feature(f, TARGET_FRAMES)\n",
    "                feats.append(f)\n",
    "                labels_part.append(lbl)\n",
    "\n",
    "            X_part = np.array(feats)[..., np.newaxis]\n",
    "            y_part = np.array(le_local.transform(labels_part))\n",
    "\n",
    "            save_path = part_paths[part_idx]\n",
    "            np.savez_compressed(save_path, X=X_part, y=y_part)\n",
    "            total_samples += X_part.shape[0]\n",
    "            print(f'  ‚Üí Guardada parte {part_idx+1}/{N_PARTS} en {save_path} (X.shape={X_part.shape})')\n",
    "\n",
    "        # Guardar metadata asociada al cache\n",
    "        metadata = {\n",
    "            \"feature_type\": ftype,\n",
    "            \"base_path\": cached_paths[ftype]['path'],\n",
    "            \"num_parts\": N_PARTS,\n",
    "            \"parts\": part_paths,\n",
    "            \"num_samples\": total_samples,\n",
    "            \"classes\": list(le_local.classes_)\n",
    "        }\n",
    "\n",
    "        with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metadata, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f'‚Üí Guardado cache en partes y metadata para {ftype} en {meta_path}')\n",
    "\n",
    "    print(\"cargado cacheo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d126ae-474d-42dc-a327-b80317fe85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo de CCN\n",
    "def build_cnn(input_shape, num_classes, dropout=DROPOUT, lr=LEARNING_RATE):\n",
    "    model = Sequential([\n",
    "        Conv2D(16, (3,3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(dropout),\n",
    "\n",
    "        Conv2D(32, (3,3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(dropout),\n",
    "\n",
    "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(dropout),\n",
    "\n",
    "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Dropout(dropout),\n",
    "\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout),\n",
    "        Dense(len(class_names), activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "print(\"funcion cargada correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae4f4cb-e8e1-4a0b-94a2-614c3f8a65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = {}\n",
    "global_start = time.time()\n",
    "\n",
    "# reproducibilidad (mantener la variable RANDOM_STATE si ya existe)\n",
    "SEED = RANDOM_STATE if 'RANDOM_STATE' in globals() else 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# silenciar warnings molestos\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Definir n√∫mero de clases antes del loop (asumimos `le` existe en el entorno como antes)\n",
    "class_names = list(le.classes_)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "for ftype in FEATURE_TYPES:\n",
    "    print('\\n' + '='*80)\n",
    "    print(f'  ENTRENANDO FEATURE TYPE: {ftype.upper()} ')\n",
    "    print('='*80)\n",
    "    \n",
    "    feature_start = time.time()\n",
    "    \n",
    "    entry = cached_paths[ftype]\n",
    "    X_all, y_all = entry['X'], entry['y']\n",
    "    \n",
    "    # Crear carpetas por feature type (manteniendo nombres existentes)\n",
    "    out_ft = os.path.join(OUTPUT_ROOT, ftype)\n",
    "    models_dir = ensure_dir(os.path.join(out_ft, 'models'))\n",
    "    reports_dir = ensure_dir(os.path.join(out_ft, 'reports'))\n",
    "    histories_dir = ensure_dir(os.path.join(out_ft, 'histories'))\n",
    "    cm_dir = ensure_dir(os.path.join(out_ft, 'confusion_matrices'))\n",
    "    curves_dir = ensure_dir(os.path.join(out_ft, 'learning_curves'))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    fold_metrics = []\n",
    "    \n",
    "    # Mostrar arquitectura antes de los folds (ejemplo)\n",
    "    try:\n",
    "        model_example = build_cnn(X_all.shape[1:], num_classes)\n",
    "        print(\"\\nüîπ Arquitectura de la CNN para feature type:\", ftype)\n",
    "        model_example.summary()\n",
    "        plot_model(model_example, to_file=os.path.join(out_ft, f'{ftype}_CNN_architecture.png'),\n",
    "                   show_shapes=True, show_layer_names=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è No se pudo construir/guardar arquitectura de ejemplo para {ftype}: {e}\")\n",
    "    \n",
    "    print(f'Feature Type \"{ftype}\" tiene {X_all.shape[0]} muestras con {num_classes} clases.')\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in tqdm(\n",
    "        enumerate(skf.split(X_all, y_all), 1),\n",
    "        total=N_FOLDS,\n",
    "        desc=f'Training {ftype.upper()}',\n",
    "        unit='fold',\n",
    "        ncols=100\n",
    "    ):\n",
    "        print(f'\\nüîπ Fold {fold}/{N_FOLDS}')\n",
    "        fold_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            X_train, X_test = X_all[train_idx], X_all[test_idx]\n",
    "            y_train, y_test = y_all[train_idx], y_all[test_idx]\n",
    "            \n",
    "            # Normalizaci√≥n per-dataset (manteniendo variable names)\n",
    "            X_train, X_test = X_train.astype('float32'), X_test.astype('float32')\n",
    "            mean, std = X_train.mean(), X_train.std() + 1e-9\n",
    "            X_train, X_test = (X_train - mean)/std, (X_test - mean)/std\n",
    "            \n",
    "            model = build_cnn(X_train.shape[1:], num_classes)\n",
    "            modelfn = os.path.join(models_dir, f'{ftype}_fold{fold}_best.h5')\n",
    "            \n",
    "            callbacks = [\n",
    "                ModelCheckpoint(modelfn, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "            ]\n",
    "            \n",
    "            print(f'Entrenando modelo para \"{ftype}\" - Fold {fold}...')\n",
    "            hist_start = time.time()\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "            hist_time = time.time() - hist_start\n",
    "            print(f'Tiempo de entrenamiento fold {fold}: {hist_time:.2f}s ({hist_time/60:.2f} min)')\n",
    "            \n",
    "            # Guardar historial por fold\n",
    "            hist_df = pd.DataFrame(history.history)\n",
    "            hist_df.to_csv(os.path.join(histories_dir, f'{ftype}_fold{fold}_history.csv'), index=False)\n",
    "            \n",
    "            # Evaluaci√≥n usando el mejor modelo guardado (si existe)\n",
    "            if os.path.exists(modelfn):\n",
    "                best_model = tf.keras.models.load_model(modelfn)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Modelo guardado no encontrado, usando modelo actual en memoria.\")\n",
    "                best_model = model\n",
    "            \n",
    "            preds = np.argmax(best_model.predict(X_test, verbose=0), axis=1)\n",
    "            \n",
    "            acc = float(accuracy_score(y_test, preds))\n",
    "            f1 = float(f1_score(y_test, preds, average='macro'))\n",
    "            prec = float(precision_score(y_test, preds, average='macro', zero_division=0))\n",
    "            rec = float(recall_score(y_test, preds, average='macro', zero_division=0))\n",
    "            \n",
    "            print(f'Resultados fold {fold}: acc={acc:.4f} f1={f1:.4f} prec={prec:.4f} rec={rec:.4f}')\n",
    "            print(f'Tiempo total fold {fold}: {time.time() - fold_start:.2f}s ({(time.time()-fold_start)/60:.2f} min)')\n",
    "            \n",
    "            # Reporte de clasificaci√≥n y matriz confusi√≥n\n",
    "            class_rep = classification_report(y_test, preds, target_names=class_names, output_dict=True)\n",
    "            with open(os.path.join(reports_dir, f'{ftype}_fold{fold}_classification.json'), 'w') as fh:\n",
    "                json.dump(class_rep, fh, indent=2)\n",
    "            \n",
    "            cm = confusion_matrix(y_test, preds)\n",
    "            plt.figure(figsize=(10,8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "            plt.title(f'Confusion Matrix - {ftype} Fold {fold}')\n",
    "            plt.ylabel('True')\n",
    "            plt.xlabel('Pred')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(cm_dir, f'{ftype}_fold{fold}_cm.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            fold_metrics.append({'fold': fold, 'acc': acc, 'f1': f1, 'prec': prec, 'rec': rec})\n",
    "        \n",
    "        except Exception as err_fold:\n",
    "            # Si un fold falla, lo registramos y continuamos con los siguientes\n",
    "            print(f\"‚ùó Error en fold {fold} de {ftype}: {err_fold}\")\n",
    "            # Guardar registro de error como archivo para debug\n",
    "            with open(os.path.join(out_ft, f'fold{fold}_error.txt'), 'w') as fh:\n",
    "                fh.write(str(err_fold))\n",
    "            continue\n",
    "    \n",
    "    # Estad√≠sticas finales por feature\n",
    "    dfm = pd.DataFrame(fold_metrics)\n",
    "    if dfm.shape[0] > 0:\n",
    "        summary = {\n",
    "            'feature_type': ftype,\n",
    "            'mean_acc': float(dfm['acc'].mean()),\n",
    "            'std_acc': float(dfm['acc'].std(ddof=0)),   # ddof=0 para poblacional; cambia si prefieres ddof=1\n",
    "            'mean_f1_macro': float(dfm['f1'].mean()),\n",
    "            'std_f1_macro': float(dfm['f1'].std(ddof=0)),\n",
    "            'per_fold': fold_metrics\n",
    "        }\n",
    "    else:\n",
    "        summary = {\n",
    "            'feature_type': ftype,\n",
    "            'mean_acc': None,\n",
    "            'std_acc': None,\n",
    "            'mean_f1_macro': None,\n",
    "            'std_f1_macro': None,\n",
    "            'per_fold': fold_metrics\n",
    "        }\n",
    "    \n",
    "    # Curvas de aprendizaje promedio (accuracy + loss)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        hist_file = os.path.join(histories_dir, f'{ftype}_fold{fold_idx+1}_history.csv')\n",
    "        if os.path.exists(hist_file):\n",
    "            h = pd.read_csv(hist_file)\n",
    "            if 'accuracy' in h.columns:\n",
    "                plt.plot(h['accuracy'], label=f'Fold {fold_idx+1}')\n",
    "            elif 'acc' in h.columns:\n",
    "                plt.plot(h['acc'], label=f'Fold {fold_idx+1}')\n",
    "    plt.title(f'Accuracy por fold - {ftype}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        hist_file = os.path.join(histories_dir, f'{ftype}_fold{fold_idx+1}_history.csv')\n",
    "        if os.path.exists(hist_file):\n",
    "            h = pd.read_csv(hist_file)\n",
    "            if 'loss' in h.columns:\n",
    "                plt.plot(h['loss'], label=f'Fold {fold_idx+1}')\n",
    "    plt.title(f'Loss por fold - {ftype}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(curves_dir, f'{ftype}_learning_curves.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    results_summary[ftype] = summary\n",
    "    dfm.to_csv(os.path.join(out_ft, 'fold_metrics.csv'), index=False)\n",
    "    with open(os.path.join(out_ft, 'summary.json'), 'w') as fh:\n",
    "        json.dump(summary, fh, indent=2)\n",
    "    \n",
    "    total_feature_time = time.time() - feature_start\n",
    "    print(f'\\n Feature \"{ftype}\" completado en {total_feature_time:.2f}s ({total_feature_time/60:.2f} min)\\n')\n",
    "\n",
    "# Comparaci√≥n general y guardado\n",
    "comp_df = pd.DataFrame([\n",
    "    {'feature': k,\n",
    "     'mean_acc': v.get('mean_acc', None),\n",
    "     'std_acc': v.get('std_acc', None),\n",
    "     'mean_f1_macro': v.get('mean_f1_macro', None),\n",
    "     'std_f1_macro': v.get('std_f1_macro', None)}\n",
    "    for k,v in results_summary.items()\n",
    "])\n",
    "comp_df.to_csv(os.path.join(OUTPUT_ROOT, 'comparison_summary.csv'), index=False)\n",
    "\n",
    "total_time = time.time() - global_start\n",
    "print(f\"\\nENTRENAMIENTO COMPLETADO en {total_time:.2f}s ({total_time/60:.2f} min)\")\n",
    "print(\"Resultados guardados en:\", OUTPUT_ROOT)\n",
    "\n",
    "# --- IMPRESI√ìN FINAL: media y desviaci√≥n est√°ndar de los k-folds por feature ---\n",
    "print(\"\\n=== RESUMEN FINAL (accuracy por feature) ===\")\n",
    "for ftype, info in results_summary.items():\n",
    "    mean_acc = info.get('mean_acc')\n",
    "    std_acc = info.get('std_acc')\n",
    "    if mean_acc is not None:\n",
    "        print(f'- {ftype}: mean_acc = {mean_acc:.4f}, std_acc = {std_acc:.4f}')\n",
    "    else:\n",
    "        print(f'- {ftype}: No se obtuvieron folds v√°lidos.')\n",
    "\n",
    "# --- GRAFICO COMPARATIVO ENTRE LAS 3 EXTRACCIONES (mfcc, mel, concat) ---\n",
    "# Filtrar s√≥lo las 3 que pidi√≥ (si est√°n presentes)\n",
    "features_to_plot = [f for f in ['mfcc', 'mel', 'concat'] if f in results_summary]\n",
    "if len(features_to_plot) == 0:\n",
    "    print(\"‚ö†Ô∏è Ninguna de las features 'mfcc', 'mel' o 'concat' est√° presente en results_summary. No se generar√° el gr√°fico comparativo.\")\n",
    "else:\n",
    "    means = [results_summary[f]['mean_acc'] for f in features_to_plot]\n",
    "    stds = [results_summary[f]['std_acc'] for f in features_to_plot]\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    x_pos = np.arange(len(features_to_plot))\n",
    "    plt.bar(x_pos, means, yerr=stds, capsize=8)\n",
    "    plt.xticks(x_pos, features_to_plot)\n",
    "    plt.ylabel('Mean Accuracy')\n",
    "    plt.xlabel('Feature Type')\n",
    "    plt.title('Comparaci√≥n: Mean Accuracy ¬± Std (k-folds)')\n",
    "    for i, v in enumerate(means):\n",
    "        plt.text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    comparison_path = os.path.join(OUTPUT_ROOT, 'feature_mean_comparison_mfcc_mel_concat.png')\n",
    "    plt.savefig(comparison_path)\n",
    "    plt.close()\n",
    "    print(f\"Gr√°fico comparativo guardado en: {comparison_path}\")\n",
    "\n",
    "# Guardar summary completo de results_summary\n",
    "with open(os.path.join(OUTPUT_ROOT, 'results_summary_all.json'), 'w') as fh:\n",
    "    json.dump(results_summary, fh, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c9590-6cc4-4e3b-aa4e-c33b85d7e25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
